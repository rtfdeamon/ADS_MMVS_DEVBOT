#!/usr/bin/env python3
"""
–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ—Å–µ—â–∞–µ–º–æ—Å—Ç–∏ —Å–∞–π—Ç–∞
"""
import sys
import json
from pathlib import Path
from datetime import datetime
from collections import defaultdict

sys.path.insert(0, str(Path(__file__).parent))

from data_collector import MetrikaDataCollector
from connector import YandexMetrikaConnector
from oauth import YandexMetrikaOAuth


def load_latest_data():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    data_dir = Path(__file__).parent / 'data'
    json_files = sorted(data_dir.glob('yandex_metrika_data_*.json'), reverse=True)
    
    if not json_files:
        print("–î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–æ–±–∏—Ä–∞—é –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")
        return None
    
    latest_file = json_files[0]
    print(f"–ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑: {latest_file.name}")
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def analyze_traffic(data):
    """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–∞—Ñ–∏–∫–∞"""
    print("\n" + "="*70)
    print(" –ê–ù–ê–õ–ò–ó –¢–†–ê–§–ò–ö–ê")
    print("="*70)
    
    visits_report = data.get('visits_report', {})
    visits_data = visits_report.get('data', [])
    
    if not visits_data:
        print("‚ö† –î–∞–Ω–Ω—ã–µ –æ –≤–∏–∑–∏—Ç–∞—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
        return
    
    # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–Ω—è–º
    daily_stats = []
    total_visits = 0
    total_pageviews = 0
    total_users = 0
    total_bounce = 0
    total_depth = 0
    total_duration = 0
    
    for row in visits_data:
        dimensions = row.get('dimensions', [])
        metrics = row.get('metrics', [])
        
        if dimensions and len(metrics) >= 6:
            date = dimensions[0].get('name', 'Unknown')
            visits = float(metrics[0]) if metrics[0] else 0
            pageviews = float(metrics[1]) if metrics[1] else 0
            users = float(metrics[2]) if metrics[2] else 0
            bounce_rate = float(metrics[3]) if metrics[3] else 0
            page_depth = float(metrics[4]) if metrics[4] else 0
            duration = float(metrics[5]) if metrics[5] else 0
            
            daily_stats.append({
                'date': date,
                'visits': visits,
                'pageviews': pageviews,
                'users': users,
                'bounce_rate': bounce_rate,
                'page_depth': page_depth,
                'duration': duration
            })
            
            total_visits += visits
            total_pageviews += pageviews
            total_users += users
            total_bounce += bounce_rate
            total_depth += page_depth
            total_duration += duration
    
    days_count = len(daily_stats)
    
    print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê (–∑–∞ {days_count} –¥–Ω–µ–π):")
    print(f"  ‚Ä¢ –í–∏–∑–∏—Ç–æ–≤: {total_visits:.0f}")
    print(f"  ‚Ä¢ –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pageviews:.0f}")
    print(f"  ‚Ä¢ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ—Å–µ—Ç–∏—Ç–µ–ª–µ–π: {total_users:.0f}")
    print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –æ—Ç–∫–∞–∑–æ–≤: {(total_bounce/days_count):.1f}%")
    print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –≥–ª—É–±–∏–Ω–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞: {(total_depth/days_count):.2f} —Å—Ç—Ä–∞–Ω–∏—Ü")
    print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∏–∑–∏—Ç–∞: {(total_duration/days_count):.0f} —Å–µ–∫ ({(total_duration/days_count/60):.1f} –º–∏–Ω)")
    
    # –ê–Ω–∞–ª–∏–∑ –¥–∏–Ω–∞–º–∏–∫–∏
    if len(daily_stats) > 1:
        print(f"\nüìà –î–ò–ù–ê–ú–ò–ö–ê:")
        first_day = daily_stats[0]
        last_day = daily_stats[-1]
        
        visits_change = ((last_day['visits'] - first_day['visits']) / first_day['visits'] * 100) if first_day['visits'] > 0 else 0
        users_change = ((last_day['users'] - first_day['users']) / first_day['users'] * 100) if first_day['users'] > 0 else 0
        
        print(f"  ‚Ä¢ –í–∏–∑–∏—Ç—ã: {first_day['visits']:.0f} ‚Üí {last_day['visits']:.0f} ({visits_change:+.1f}%)")
        print(f"  ‚Ä¢ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏: {first_day['users']:.0f} ‚Üí {last_day['users']:.0f} ({users_change:+.1f}%)")
        
        # –õ—É—á—à–∏–π –∏ —Ö—É–¥—à–∏–π –¥–µ–Ω—å
        best_day = max(daily_stats, key=lambda x: x['visits'])
        worst_day = min(daily_stats, key=lambda x: x['visits'])
        print(f"\n  ‚Ä¢ –õ—É—á—à–∏–π –¥–µ–Ω—å: {best_day['date']} ({best_day['visits']:.0f} –≤–∏–∑–∏—Ç–æ–≤)")
        print(f"  ‚Ä¢ –•—É–¥—à–∏–π –¥–µ–Ω—å: {worst_day['date']} ({worst_day['visits']:.0f} –≤–∏–∑–∏—Ç–æ–≤)")
    
    return daily_stats


def analyze_sources(data):
    """–ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ç—Ä–∞—Ñ–∏–∫–∞"""
    print("\n" + "="*70)
    print(" –ê–ù–ê–õ–ò–ó –ò–°–¢–û–ß–ù–ò–ö–û–í –¢–†–ê–§–ò–ö–ê")
    print("="*70)
    
    sources_report = data.get('sources_report', {})
    sources_data = sources_report.get('data', [])
    
    if not sources_data:
        print("‚ö† –î–∞–Ω–Ω—ã–µ –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
        return
    
    sources_dict = defaultdict(lambda: {'visits': 0, 'pageviews': 0, 'users': 0})
    engines_dict = defaultdict(lambda: {'visits': 0, 'pageviews': 0, 'users': 0})
    
    for row in sources_data:
        dimensions = row.get('dimensions', [])
        metrics = row.get('metrics', [])
        
        if dimensions and len(metrics) >= 3:
            source = dimensions[0].get('name', 'Unknown')
            engine = dimensions[1].get('name', 'Unknown') if len(dimensions) > 1 else 'Unknown'
            
            visits = float(metrics[0]) if metrics[0] else 0
            pageviews = float(metrics[1]) if metrics[1] else 0
            users = float(metrics[2]) if metrics[2] else 0
            
            sources_dict[source]['visits'] += visits
            sources_dict[source]['pageviews'] += pageviews
            sources_dict[source]['users'] += users
            
            if engine != 'Unknown':
                engines_dict[engine]['visits'] += visits
                engines_dict[engine]['pageviews'] += pageviews
                engines_dict[engine]['users'] += users
    
    total_visits = sum(s['visits'] for s in sources_dict.values())
    
    print(f"\nüìä –¢–û–ü –ò–°–¢–û–ß–ù–ò–ö–û–í –¢–†–ê–§–ò–ö–ê:")
    sorted_sources = sorted(sources_dict.items(), key=lambda x: x[1]['visits'], reverse=True)
    
    for i, (source, stats) in enumerate(sorted_sources[:10], 1):
        share = (stats['visits'] / total_visits * 100) if total_visits > 0 else 0
        print(f"  {i:2}. {source:30} | –í–∏–∑–∏—Ç–æ–≤: {stats['visits']:6.0f} ({share:5.1f}%) | –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {stats['pageviews']:6.0f}")
    
    if engines_dict:
        print(f"\nüîç –ü–û–ò–°–ö–û–í–´–ï –°–ò–°–¢–ï–ú–´:")
        sorted_engines = sorted(engines_dict.items(), key=lambda x: x[1]['visits'], reverse=True)
        for engine, stats in sorted_engines[:5]:
            if engine and engine != 'Unknown':
                share = (stats['visits'] / total_visits * 100) if total_visits > 0 else 0
                engine_name = str(engine) if engine else 'Unknown'
                print(f"  ‚Ä¢ {engine_name:20} | –í–∏–∑–∏—Ç–æ–≤: {stats['visits']:6.0f} ({share:5.1f}%)")


def analyze_pages(data):
    """–ê–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
    print("\n" + "="*70)
    print(" –ê–ù–ê–õ–ò–ó –ü–û–ü–£–õ–Ø–†–ù–´–• –°–¢–†–ê–ù–ò–¶")
    print("="*70)
    
    pages_report = data.get('pages_report', {})
    pages_data = pages_report.get('data', [])
    
    if not pages_data:
        print("‚ö† –î–∞–Ω–Ω—ã–µ –æ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
        return
    
    pages_list = []
    for row in pages_data:
        dimensions = row.get('dimensions', [])
        metrics = row.get('metrics', [])
        
        if dimensions and len(metrics) >= 2:
            url = dimensions[0].get('name', 'Unknown')
            title = dimensions[1].get('name', 'Unknown') if len(dimensions) > 1 else 'Unknown'
            
            pageviews = float(metrics[0]) if metrics[0] else 0
            users = float(metrics[1]) if metrics[1] else 0
            
            pages_list.append({
                'url': url,
                'title': title,
                'pageviews': pageviews,
                'users': users
            })
    
    sorted_pages = sorted(pages_list, key=lambda x: x['pageviews'], reverse=True)
    total_pageviews = sum(p['pageviews'] for p in pages_list)
    
    print(f"\nüìÑ –¢–û–ü-20 –°–¢–†–ê–ù–ò–¶:")
    for i, page in enumerate(sorted_pages[:20], 1):
        share = (page['pageviews'] / total_pageviews * 100) if total_pageviews > 0 else 0
        url_short = page['url'][:50] + '...' if len(page['url']) > 50 else page['url']
        print(f"  {i:2}. {url_short:52} | –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {page['pageviews']:6.0f} ({share:5.1f}%) | –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {page['users']:5.0f}")


def analyze_geo(data):
    """–ê–Ω–∞–ª–∏–∑ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏"""
    print("\n" + "="*70)
    print(" –ê–ù–ê–õ–ò–ó –ì–ï–û–ì–†–ê–§–ò–ò –ü–û–°–ï–¢–ò–¢–ï–õ–ï–ô")
    print("="*70)
    
    geo_report = data.get('geo_report', {})
    geo_data = geo_report.get('data', [])
    
    if not geo_data:
        print("‚ö† –î–∞–Ω–Ω—ã–µ –æ –≥–µ–æ–≥—Ä–∞—Ñ–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
        return
    
    countries_dict = defaultdict(lambda: {'visits': 0, 'pageviews': 0, 'users': 0})
    cities_list = []
    
    for row in geo_data:
        dimensions = row.get('dimensions', [])
        metrics = row.get('metrics', [])
        
        if dimensions and len(metrics) >= 3:
            country = dimensions[0].get('name', 'Unknown')
            city = dimensions[1].get('name', 'Unknown') if len(dimensions) > 1 else 'Unknown'
            
            visits = float(metrics[0]) if metrics[0] else 0
            pageviews = float(metrics[1]) if metrics[1] else 0
            users = float(metrics[2]) if metrics[2] else 0
            
            countries_dict[country]['visits'] += visits
            countries_dict[country]['pageviews'] += pageviews
            countries_dict[country]['users'] += users
            
            cities_list.append({
                'country': country,
                'city': city,
                'visits': visits,
                'pageviews': pageviews,
                'users': users
            })
    
    total_visits = sum(c['visits'] for c in countries_dict.values())
    
    print(f"\nüåç –¢–û–ü –°–¢–†–ê–ù:")
    sorted_countries = sorted(countries_dict.items(), key=lambda x: x[1]['visits'], reverse=True)
    for i, (country, stats) in enumerate(sorted_countries[:10], 1):
        share = (stats['visits'] / total_visits * 100) if total_visits > 0 else 0
        print(f"  {i:2}. {country:30} | –í–∏–∑–∏—Ç–æ–≤: {stats['visits']:6.0f} ({share:5.1f}%) | –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {stats['users']:5.0f}")
    
    print(f"\nüèôÔ∏è  –¢–û–ü –ì–û–†–û–î–û–í:")
    sorted_cities = sorted(cities_list, key=lambda x: x['visits'], reverse=True)
    for i, city_data in enumerate(sorted_cities[:15], 1):
        share = (city_data['visits'] / total_visits * 100) if total_visits > 0 else 0
        city = str(city_data['city']) if city_data['city'] else 'Unknown'
        country = str(city_data['country']) if city_data['country'] else 'Unknown'
        print(f"  {i:2}. {city:30} ({country:15}) | –í–∏–∑–∏—Ç–æ–≤: {city_data['visits']:6.0f} ({share:5.1f}%)")


def generate_conclusions(data, daily_stats):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã–≤–æ–¥–æ–≤ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    print("\n" + "="*70)
    print(" –í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
    print("="*70)
    
    counter_info = data.get('counter_info', {})
    site_name = counter_info.get('name', '–°–∞–π—Ç')
    site_url = counter_info.get('site', 'N/A')
    
    print(f"\nüìã –ê–ù–ê–õ–ò–ó –î–õ–Ø: {site_name} ({site_url})")
    
    conclusions = []
    recommendations = []
    
    # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–∞—Ñ–∏–∫–∞
    if daily_stats:
        avg_visits = sum(d['visits'] for d in daily_stats) / len(daily_stats)
        avg_bounce = sum(d['bounce_rate'] for d in daily_stats) / len(daily_stats)
        avg_depth = sum(d['page_depth'] for d in daily_stats) / len(daily_stats)
        avg_duration = sum(d['duration'] for d in daily_stats) / len(daily_stats)
        
        conclusions.append(f"–°—Ä–µ–¥–Ω—è—è –ø–æ—Å–µ—â–∞–µ–º–æ—Å—Ç—å: {avg_visits:.0f} –≤–∏–∑–∏—Ç–æ–≤ –≤ –¥–µ–Ω—å")
        
        if avg_bounce > 70:
            conclusions.append(f"‚ö†Ô∏è  –í—ã—Å–æ–∫–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –æ—Ç–∫–∞–∑–æ–≤: {avg_bounce:.1f}%")
            recommendations.append("–£–ª—É—á—à–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å–∞–¥–æ—á–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
        elif avg_bounce < 40:
            conclusions.append(f"‚úì –ù–∏–∑–∫–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –æ—Ç–∫–∞–∑–æ–≤: {avg_bounce:.1f}% (–æ—Ç–ª–∏—á–Ω–æ!)")
        
        if avg_depth < 2:
            conclusions.append(f"‚ö†Ô∏è  –ù–∏–∑–∫–∞—è –≥–ª—É–±–∏–Ω–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞: {avg_depth:.2f} —Å—Ç—Ä–∞–Ω–∏—Ü")
            recommendations.append("–î–æ–±–∞–≤–∏—Ç—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏, —É–ª—É—á—à–∏—Ç—å –Ω–∞–≤–∏–≥–∞—Ü–∏—é, —Å–æ–∑–¥–∞—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç")
        elif avg_depth > 3:
            conclusions.append(f"‚úì –•–æ—Ä–æ—à–∞—è –≥–ª—É–±–∏–Ω–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞: {avg_depth:.2f} —Å—Ç—Ä–∞–Ω–∏—Ü")
        
        if avg_duration < 60:
            conclusions.append(f"‚ö†Ô∏è  –ö–æ—Ä–æ—Ç–∫–æ–µ –≤—Ä–µ–º—è –Ω–∞ —Å–∞–π—Ç–µ: {avg_duration:.0f} —Å–µ–∫")
            recommendations.append("–£–ª—É—á—à–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç, –¥–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã, –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏")
        elif avg_duration > 180:
            conclusions.append(f"‚úì –•–æ—Ä–æ—à–µ–µ –≤—Ä–µ–º—è –Ω–∞ —Å–∞–π—Ç–µ: {avg_duration:.0f} —Å–µ–∫")
    
    # –ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    sources_report = data.get('sources_report', {})
    sources_data = sources_report.get('data', [])
    if sources_data:
        search_visits = 0
        direct_visits = 0
        total_visits = 0
        
        for row in sources_data:
            dimensions = row.get('dimensions', [])
            metrics = row.get('metrics', [])
            if dimensions and metrics:
                source = dimensions[0].get('name', '').lower()
                visits = float(metrics[0]) if metrics[0] else 0
                total_visits += visits
                
                if 'search' in source or '–ø–æ–∏—Å–∫' in source:
                    search_visits += visits
                elif 'direct' in source or '–ø—Ä—è–º–æ–π' in source or 'none' in source:
                    direct_visits += visits
        
        if total_visits > 0:
            search_share = (search_visits / total_visits) * 100
            direct_share = (direct_visits / total_visits) * 100
            
            conclusions.append(f"–ü–æ–∏—Å–∫–æ–≤—ã–π —Ç—Ä–∞—Ñ–∏–∫: {search_share:.1f}%")
            conclusions.append(f"–ü—Ä—è–º–æ–π —Ç—Ä–∞—Ñ–∏–∫: {direct_share:.1f}%")
            
            if search_share < 30:
                recommendations.append("–£—Å–∏–ª–∏—Ç—å SEO-–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∏ —Ä–∞–±–æ—Ç—É —Å –ø–æ–∏—Å–∫–æ–≤—ã–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏")
            if direct_share > 50:
                conclusions.append("‚úì –í—ã—Å–æ–∫–∞—è –¥–æ–ª—è –ø—Ä—è–º–æ–≥–æ —Ç—Ä–∞—Ñ–∏–∫–∞ - —Ö–æ—Ä–æ—à–∞—è —É–∑–Ω–∞–≤–∞–µ–º–æ—Å—Ç—å –±—Ä–µ–Ω–¥–∞")
    
    print("\nüìä –í–´–í–û–î–´:")
    for i, conclusion in enumerate(conclusions, 1):
        print(f"  {i}. {conclusion}")
    
    if recommendations:
        print("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
        for i, rec in enumerate(recommendations, 1):
            print(f"  {i}. {rec}")
    else:
        print("\n‚úì –ü–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –≤ –Ω–æ—Ä–º–µ. –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é.")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("\n" + "="*70)
    print(" –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–û–°–ï–©–ê–ï–ú–û–°–¢–ò")
    print("="*70)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    data = load_latest_data()
    
    if not data:
        # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        print("\n–°–æ–±–∏—Ä–∞—é –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")
        oauth = YandexMetrikaOAuth()
        token = oauth.get_valid_token()
        
        if not token:
            print("‚úó –¢–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—é.")
            return
        
        connector = YandexMetrikaConnector(token=token)
        collector = MetrikaDataCollector(connector)
        
        counters = connector.get_counters()
        if not counters:
            print("‚úó –°—á–µ—Ç—á–∏–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        counter_id = counters[0]['id']
        data = collector.collect_all_data(counter_id=counter_id)
        collector.save_data(data)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
    daily_stats = analyze_traffic(data)
    analyze_sources(data)
    analyze_pages(data)
    analyze_geo(data)
    generate_conclusions(data, daily_stats)
    
    print("\n" + "="*70)
    print(" –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù")
    print("="*70)


if __name__ == '__main__':
    main()

